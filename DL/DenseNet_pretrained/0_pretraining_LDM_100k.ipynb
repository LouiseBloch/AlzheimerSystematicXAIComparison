{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d49e480",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import logging\n",
    "from livelossplot import PlotLosses\n",
    "import pickle\n",
    "import torch\n",
    "import monai\n",
    "import time\n",
    "from monai.data import DataLoader\n",
    "from monai.transforms import (\n",
    "    AddChanneld,\n",
    "    CenterSpatialCropd,\n",
    "    Compose,\n",
    "    Resized,\n",
    "    RandSpatialCropd,\n",
    "    ScaleIntensityd,\n",
    "    ToTensord,\n",
    "    LoadImaged,\n",
    "    Identityd,\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from monai.utils import InterpolateMode\n",
    "import nibabel as nib\n",
    "import lime.lime_tabular\n",
    "from skimage.segmentation import slic\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "from sklearn.model_selection import KFold\n",
    "from torchmetrics import MeanMetric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5191bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt=\"adam\"\n",
    "lr=1e-2\n",
    "BATCH_SIZE=12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1583458",
   "metadata": {},
   "outputs": [],
   "source": [
    "#definitions of paths\n",
    "RESULTS_DIR = os.path.join(\"./DenseNet_pretrained/\")\n",
    "MODEL_DIR = os.path.join(\"./DenseNet_pretrained/pretraining_model/\")\n",
    "path_train_data=os.path.join(\"../../data/LDM_DL_train.csv\")\n",
    "path_valid_data=os.path.join(\"../../data/LDM_DL_valid.csv\")\n",
    "filenameCSV=os.path.join(\"./DenseNet_pretrained/pretraining_model/LDM_Results_DenseNet.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81115f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if model directory not exists create model directory\n",
    "if not os.path.exists(RESULTS_DIR):\n",
    "    os.makedirs(RESULTS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bf680f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if model directory not exists create model directory\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.makedirs(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a158ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read training and validation dataset\n",
    "train=pd.read_csv(path_train_data,index_col=\"participant_id\")\n",
    "valid=pd.read_csv(path_valid_data,index_col=\"participant_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9740e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data augmentations\n",
    "train_transforms = Compose(\n",
    "        [\n",
    "            LoadImaged(keys=[\"img\"]),\n",
    "            AddChanneld(keys=[\"img\"]),\n",
    "            ScaleIntensityd(keys=[\"img\"]),\n",
    "            Resized(keys=[\"img\"],spatial_size=(256,256,256)),\n",
    "            RandSpatialCropd(keys=[\"img\"],roi_size=(224,224,224),random_size =False),\n",
    "            ToTensord(keys=[\"img\"]),\n",
    "        ]\n",
    "    )\n",
    "valid_transforms = Compose(\n",
    "        [\n",
    "            LoadImaged(keys=[\"img\"]),\n",
    "            AddChanneld(keys=[\"img\"]),\n",
    "            ScaleIntensityd(keys=[\"img\"]),\n",
    "            Resized(keys=[\"img\"],spatial_size=(256,256,256)),\n",
    "            CenterSpatialCropd(keys=[\"img\"],roi_size=(224,224,224)),\n",
    "            ToTensord(keys=[\"img\"]),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f87ed69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function to set seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2debbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "liveloss = PlotLosses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6091ddcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = MeanMetric()\n",
    "#normalize output value by mean and std of training dataset\n",
    "sdt_age_train=train.age.std()\n",
    "mean_age_train=train.age.mean()\n",
    "train.loc[:,\"age\"]=(train.loc[:,\"age\"]-mean_age_train)/sdt_age_train\n",
    "valid.loc[:,\"age\"]=(valid.loc[:,\"age\"]-mean_age_train)/sdt_age_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cfbc74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform training and validation datasets to pytorch format\n",
    "trainDSNew = [{\"img\": img, \"age\":age} for img,age in zip(train.filename,train.age)]\n",
    "validDSNew = [{\"img\": img, \"age\":age} for img,age in zip(valid.filename,valid.age)]\n",
    "set_seed(123)\n",
    "train_ds = monai.data.Dataset(data=trainDSNew, transform=train_transforms)\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=8, pin_memory=torch.cuda.is_available())\n",
    "valid_ds = monai.data.Dataset(data=validDSNew, transform=valid_transforms)\n",
    "valid_loader = DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=8, pin_memory=torch.cuda.is_available())\n",
    "#enable mixed precision to increase batch size\n",
    "use_amp = True\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "#define batchsize factor used for batch accumulation --> the virtual batch size is thus 120\n",
    "batchsize_factor=120//BATCH_SIZE\n",
    "#define maximum number of epochs\n",
    "max_epochs = 50\n",
    "#set seed for reproducibility\n",
    "set_seed(123)\n",
    "#choose cuda as the device if it is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#load DL model for regression using monai\n",
    "model=monai.networks.nets.densenet121(spatial_dims=3, in_channels=1, out_channels=1)\n",
    "#define MSELoss as regression loss function\n",
    "loss_function = torch.nn.MSELoss()\n",
    "#select optimizer used for training\n",
    "if opt ==\"adam\":\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "elif opt==\"sgd\":\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "else:\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
    "#iterate over epochs\n",
    "for epoch in range(max_epochs):\n",
    "    #initalize starting time of epoch\n",
    "    start = time.time()\n",
    "    #start with model in train mode\n",
    "    model.train()\n",
    "    logs = {}\n",
    "    epoch_loss = 0\n",
    "    epoch_loss_val = 0\n",
    "    step = 0\n",
    "    #iterate over training batches\n",
    "    for batch_data in train_loader:\n",
    "        step += 1\n",
    "        model=model.cuda()\n",
    "        #load input scans and normalized age of batch\n",
    "        inputs=batch_data[\"img\"].cuda()\n",
    "        age=batch_data[\"age\"].cuda()\n",
    "        age=age.float()\n",
    "        age=age[:,None]\n",
    "        #use mixed precision\n",
    "        with torch.cuda.amp.autocast(): \n",
    "            #calculate model predictions\n",
    "            outputs = model(inputs)\n",
    "            #calculate loss\n",
    "            loss=loss_function(outputs,age)\n",
    "        loss = loss / batchsize_factor\n",
    "        scaler.scale(loss).backward()\n",
    "        #batch accumulation\n",
    "        if ((step+1) % batchsize_factor)==0:\n",
    "            #update scaler and optimizer\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            metric.update(loss.item()*batchsize_factor)\n",
    "    #compute MSE loss for training dataset\n",
    "    epoch_loss=metric.compute()\n",
    "    metric.reset()\n",
    "    logs['log loss'] = epoch_loss.item()\n",
    "    #model validation\n",
    "    #change model to evaluation model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        #iterate over batches in validation loader\n",
    "        for batch_data in valid_loader:\n",
    "            model=model.cuda()\n",
    "            #load input scans and normalized age for batch\n",
    "            inputs=batch_data[\"img\"].cuda()\n",
    "            age=batch_data[\"age\"].cuda()\n",
    "            age=age.float()\n",
    "            age=age[:,None]\n",
    "            #use mixed precision\n",
    "            with torch.cuda.amp.autocast(): \n",
    "                #calculate model predictions\n",
    "                outputs = model(inputs)\n",
    "                #calculate loss\n",
    "                loss=loss_function(outputs,age) \n",
    "            #compute MSE loss for validation dataset\n",
    "            metric.update(loss.item())\n",
    "        epoch_loss_val =metric.compute()\n",
    "        logs['val_log loss'] = epoch_loss_val.item()\n",
    "        metric.reset()\n",
    "    liveloss.update(logs)\n",
    "    liveloss.send()\n",
    "    #save model\n",
    "    torch.save(model.state_dict(),MODEL_DIR+\"model_\"+str(opt)+\"_\"+str(lr)+\"_\"+str(epoch)+\".pth\")\n",
    "    #save model performance\n",
    "    d = {'optimizer': [opt], 'LR': [lr], 'Epoch':[epoch], \"Epoch-Loss\":[epoch_loss_val]}\n",
    "    df = pd.DataFrame(data=d)\n",
    "    if os.path.isfile(filenameCSV):\n",
    "        df.to_csv(filenameCSV, mode='a', header=False)\n",
    "    else:\n",
    "        df.to_csv(filenameCSV, mode='w', header=True)\n",
    "    end = time.time()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
