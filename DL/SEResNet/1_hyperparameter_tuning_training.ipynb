{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9801f0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import sys\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score\n",
    "import logging\n",
    "from livelossplot import PlotLosses\n",
    "\n",
    "import torch\n",
    "import monai\n",
    "from monai.data import DataLoader\n",
    "from monai.transforms import (\n",
    "    AddChanneld,\n",
    "    CenterSpatialCropd,\n",
    "    Compose,\n",
    "    Resized,\n",
    "    RandSpatialCropd,\n",
    "    ScaleIntensityd,\n",
    "    ToTensord,\n",
    "    LoadImaged,\n",
    "    Identityd,\n",
    ")\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77357ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#definitions of path\n",
    "MODEL_DIR = os.path.join(\"./SEResNet/\")\n",
    "path_train_data=os.path.join(\"../../data/trainValid_DL.csv\")\n",
    "filenameCSV=os.path.join(\"./SEResNet/hyperparameter_tuning_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6a4c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#definition of batch size, numbers should be devisors of 64\n",
    "BATCH_SIZE=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95399deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if model directory not exists create model directory\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.makedirs(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2598210",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load ADNI training dataset\n",
    "trainValidMerged=pd.read_csv(path_train_data,index_col=\"PTID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cf3ba9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16783d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data augmentations\n",
    "train_transforms = Compose(\n",
    "        [\n",
    "            LoadImaged(keys=[\"img\"]),\n",
    "            AddChanneld(keys=[\"img\"]),\n",
    "            ScaleIntensityd(keys=[\"img\"]),\n",
    "            Resized(keys=[\"img\"],spatial_size=(256,256,256)),\n",
    "            RandSpatialCropd(keys=[\"img\"],roi_size=(224,224,224),random_size =False),\n",
    "            ToTensord(keys=[\"img\"]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "valid_transforms = Compose(\n",
    "        [\n",
    "            LoadImaged(keys=[\"img\"]),\n",
    "            AddChanneld(keys=[\"img\"]),\n",
    "            ScaleIntensityd(keys=[\"img\"]),\n",
    "            Resized(keys=[\"img\"],spatial_size=(256,256,256)),\n",
    "            CenterSpatialCropd(keys=[\"img\"],roi_size=(224,224,224)),\n",
    "            ToTensord(keys=[\"img\"]),\n",
    "        ]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6696d65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define function to set seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c759518",
   "metadata": {},
   "outputs": [],
   "source": [
    "liveloss = PlotLosses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6c25c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35017eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate over all hyperparameters\n",
    "for lr in [1e-1,1e-2,1e-3,1e-4,1e-5]:\n",
    "    for opt in [\"sgd\",\"adam\",\"rmsprop\"]:\n",
    "        for strategy in [\"none\",\"exp\",\"step\"]:\n",
    "            #check if model with parameters is already trained\n",
    "            file_path=MODEL_DIR+\"model_\"+str(opt)+\"_\"+str(lr)+\"_\"+str(strategy)+\"_5_49.pth\"\n",
    "            if not os.path.isfile(file_path):\n",
    "                #define cross validation splits\n",
    "                kf = StratifiedKFold(n_splits=5,shuffle=True,random_state=101)\n",
    "                cvIt=0\n",
    "                #iterate over all cross-validation splits\n",
    "                for trainCross, validCross in kf.split(trainValidMerged,trainValidMerged.DX):\n",
    "                    cvIt+=1\n",
    "                    #extract training and validation data\n",
    "                    training=trainValidMerged.iloc[trainCross]\n",
    "                    valid=trainValidMerged.iloc[validCross]\n",
    "                    #extract diagnosis for training and validation data\n",
    "                    Y_train=pd.get_dummies(training.DX,drop_first=True).to_numpy().squeeze()\n",
    "                    Y_train=Y_train.tolist()\n",
    "                    Y_valid=pd.get_dummies(valid.DX,drop_first=True).to_numpy().squeeze()\n",
    "                    Y_valid=Y_valid.tolist()\n",
    "                    #reformat training and validation datasets for pytorch\n",
    "                    trainDSNew = [{\"img\": img, \"label\": label} for img, label in zip(training.filename, Y_train)]\n",
    "                    validDSNew = [{\"img\": img, \"label\": label} for img, label in zip(valid.filename, Y_valid)]\n",
    "                    train_ds = monai.data.Dataset(data=trainDSNew, transform=train_transforms)\n",
    "                    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=8, pin_memory=torch.cuda.is_available())\n",
    "                    valid_ds = monai.data.Dataset(data=validDSNew, transform=valid_transforms)\n",
    "                    valid_loader = DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=8, pin_memory=torch.cuda.is_available())\n",
    "                    #set seed for reproducibility\n",
    "                    set_seed(123)\n",
    "                    #disable mixed precision as there are some problems with monai models\n",
    "                    use_amp = False\n",
    "                    #define gradient scaler\n",
    "                    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "                    #define batchsize factor used for batch accumulation --> the virtual batch size is thus 64\n",
    "                    batchsize_factor=64 // BATCH_SIZE\n",
    "                    #define interval in which the validation should be performed\n",
    "                    val_interval = 1\n",
    "                    #define list to store epoch loss values and accuracy scores per epoch\n",
    "                    epoch_loss_values = []\n",
    "                    #set maximal number of epochs\n",
    "                    max_epochs = 50\n",
    "                    set_seed(123)\n",
    "                    #choose cuda as the device if it is available\n",
    "                    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "                    #load DL model using monai\n",
    "                    model = monai.networks.nets.SEResNet152(num_classes=2,spatial_dims=3, in_channels=1)\n",
    "                    #define cross entropy as loss function\n",
    "                    loss_function = torch.nn.CrossEntropyLoss()\n",
    "                    #select optimizer\n",
    "                    if opt ==\"adam\":\n",
    "                        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "                    elif opt==\"sgd\":\n",
    "                        optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "                    else:\n",
    "                        optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
    "                    #select learning rate scheduler\n",
    "                    if strategy==\"step\":\n",
    "                        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10)\n",
    "                    elif strategy==\"exp\":\n",
    "                        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9) \n",
    "                    model=model.to(device)\n",
    "                    set_seed(123)\n",
    "                    #iterate over epochs\n",
    "                    for epoch in range(max_epochs):\n",
    "                        #store starting time of epoch\n",
    "                        start = time.time()\n",
    "                        #start with model in train mode\n",
    "                        model.train()\n",
    "                        logs = {}\n",
    "                        epoch_loss = 0\n",
    "                        epoch_loss_val = 0\n",
    "                        step = 0\n",
    "                        #initialize lists to store predictions and labels of training and validation data\n",
    "                        preds_epoch=[]\n",
    "                        label_epoch=[]\n",
    "                        preds_epoch_val=[]\n",
    "                        label_epoch_val=[]\n",
    "                        #iterate over training batches\n",
    "                        for batch_data in train_loader:\n",
    "                            step += 1\n",
    "                            #load images and labels for batch\n",
    "                            inputs=batch_data[\"img\"].cuda()\n",
    "                            labels=batch_data[\"label\"].cuda()\n",
    "                            #get model output\n",
    "                            outputs = model(inputs)\n",
    "                            #calculate loss\n",
    "                            loss = loss_function(outputs, labels)/batchsize_factor\n",
    "                            scaler.scale(loss).backward()\n",
    "                            #batch accumulation\n",
    "                            if (step+1) % batchsize_factor==0:\n",
    "                                #update scaler and optimizer\n",
    "                                scaler.step(optimizer)\n",
    "                                scaler.update()\n",
    "                                optimizer.zero_grad()\n",
    "                            #calculate loss over epoch\n",
    "                            epoch_loss += (loss.item()*batchsize_factor)\n",
    "                            #extract prediction of model\n",
    "                            _, preds = torch.max(outputs, 1)\n",
    "                            #store prediction of model and label of subjects in the batch\n",
    "                            preds_epoch.append(preds.cpu().detach().numpy())\n",
    "                            label_epoch.append(labels.data.cpu().detach().numpy())\n",
    "                        #increase step for scheduler\n",
    "                        if strategy==\"step\":\n",
    "                            scheduler.step()\n",
    "                        elif strategy==\"exp\":\n",
    "                            scheduler.step()\n",
    "                        #store epoch training loss\n",
    "                        logs['log loss'] = epoch_loss\n",
    "                        #calculate and store training accuracy \n",
    "                        preds_epoch = [item for sublist in preds_epoch for item in sublist]\n",
    "                        label_epoch = [item for sublist in label_epoch for item in sublist]\n",
    "                        logs['accuracy']=accuracy_score(label_epoch,preds_epoch)*100\n",
    "                        epoch_loss_values.append(epoch_loss)\n",
    "                        #model validation\n",
    "                        if (epoch + 1) % val_interval == 0:\n",
    "                            #change model to evaluation model\n",
    "                            model.eval()\n",
    "                            with torch.no_grad():\n",
    "                                num_correct = 0.0\n",
    "                                metric_count = 0\n",
    "                                #iterate over validation data\n",
    "                                for val_data in valid_loader:\n",
    "                                    #load images and labels of validation batch\n",
    "                                    inputs=val_data[\"img\"].cuda()\n",
    "                                    labels=val_data[\"label\"].cuda()\n",
    "                                    #calculate outputs of model\n",
    "                                    outputs = model(inputs)\n",
    "                                    #calculate loss\n",
    "                                    loss=loss_function(outputs, labels)\n",
    "                                    #calculate and store prediction and label for batch\n",
    "                                    _, preds= torch.max(outputs, 1)\n",
    "                                    preds_epoch_val.append(preds.cpu().detach().numpy())\n",
    "                                    label_epoch_val.append(labels.data.cpu().detach().numpy())\n",
    "                                    epoch_loss_val += loss.item()\n",
    "                                #calculate and store epoch loss during validation and accuracy during validation\n",
    "                                logs['val_log loss'] = epoch_loss_val\n",
    "                                preds_epoch_val = [item for sublist in preds_epoch_val for item in sublist]\n",
    "                                label_epoch_val = [item for sublist in label_epoch_val for item in sublist]\n",
    "                                logs['val_accuracy']=accuracy_score(label_epoch_val,preds_epoch_val)*100\n",
    "                        #show training and validation loss and accuracy in liveloss plot\n",
    "                        liveloss.update(logs)\n",
    "                        liveloss.send()\n",
    "                        #save model parameters \n",
    "                        torch.save(model.state_dict(),MODEL_DIR+\"model_\"+str(opt)+\"_\"+str(lr)+\"_\"+str(strategy)+\"_\"+str(cvIt)+\"_\"+str(epoch)+\".pth\")\n",
    "                        #save hyperparameters and results of model\n",
    "                        d = {'optimizer': [opt], 'LR': [lr],'strategy':[strategy],'CV':[cvIt], 'Epoch':[epoch], \"Epoch-Accuracy\":[accuracy_score(label_epoch_val,preds_epoch_val)*100],\"Epoch-Loss\":[epoch_loss_val]}\n",
    "                        df = pd.DataFrame(data=d)\n",
    "                        if os.path.isfile(filenameCSV):\n",
    "                            df.to_csv(filenameCSV, mode='a', header=False)\n",
    "                        else:\n",
    "                            df.to_csv(filenameCSV, mode='w', header=True)\n",
    "                        end = time.time()\n",
    "                        #calculate time used to train one epoch\n",
    "                        print(format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5356a30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load table that includes all results received during the hyperparameter tuning\n",
    "df_hyperparameters=pd.read_csv(filenameCSV)\n",
    "#drop duplicated entries, (if the pipeline is run multiple times), for all hyperparameter combinations, the last entry is kept\n",
    "df_filtered=df_hyperparameters.drop_duplicates(subset=[\"optimizer\",\"LR\",\"strategy\",\"CV\",\"Epoch\"],keep=\"last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ec4dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculated mean and sd accuracies as well as losses for all hyperparameter combinations\n",
    "df_mean=pd.DataFrame(columns = ['optimizer', 'lr',\"strategy\", 'epoch',\"Mean ACC\",\"Mean Loss\"])\n",
    "\n",
    "for optimizer in df_filtered.optimizer.unique():\n",
    "    for lr in df_filtered.LR.unique():\n",
    "        for strategy in df_filtered.strategy.unique():\n",
    "            for epoch in df_filtered.Epoch.unique():\n",
    "                mean_value=df_filtered[((df_filtered.optimizer==optimizer)&(df_filtered.strategy==strategy)&(df_filtered.LR==lr)&(df_filtered.Epoch==epoch))][\"Epoch-Accuracy\"].mean()\n",
    "                sd_value=df_filtered[((df_filtered.optimizer==optimizer)&(df_filtered.strategy==strategy)&(df_filtered.LR==lr)&(df_filtered.Epoch==epoch))][\"Epoch-Accuracy\"].std()\n",
    "                mean_loss=df_filtered[((df_filtered.optimizer==optimizer)&(df_filtered.strategy==strategy)&(df_filtered.LR==lr)&(df_filtered.Epoch==epoch))][\"Epoch-Loss\"].mean()\n",
    "                sd_loss=df_filtered[((df_filtered.optimizer==optimizer)&(df_filtered.strategy==strategy)&(df_filtered.LR==lr)&(df_filtered.Epoch==epoch))][\"Epoch-Loss\"].std()\n",
    "                df_new=pd.DataFrame([{'optimizer' : optimizer, 'lr' : lr,'strategy': strategy, 'epoch' : epoch,\"Mean ACC\":mean_value,\"Mean Loss\":mean_loss,\"sd ACC\":sd_value,\"sd loss\":sd_loss}])\n",
    "                df_mean = pd.concat([df_mean,df_new])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5066d9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#identify hyperparameter combination that achieved the best mean CV accuraacy\n",
    "df_mean=df_mean.reset_index()\n",
    "max_idx=df_mean[\"Mean ACC\"].astype(float).idxmax()\n",
    "max_obj=df_mean.iloc[max_idx]\n",
    "optimizer=max_obj[\"optimizer\"]\n",
    "lr=max_obj[\"lr\"]\n",
    "strategy=max_obj[\"strategy\"]\n",
    "epoch=max_obj[\"epoch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c64fc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#additional training for another 50 epochs for the hyperparameters that performed best\n",
    "#define cross validation splits\n",
    "kf = StratifiedKFold(n_splits=5,shuffle=True,random_state=101)\n",
    "cvIt=0\n",
    "#iterate over all cross-validation splits\n",
    "for trainCross, validCross in kf.split(trainValidMerged,trainValidMerged.DX):\n",
    "    cvIt+=1\n",
    "    #extract training and validation data\n",
    "    training=trainValidMerged.iloc[trainCross]\n",
    "    valid=trainValidMerged.iloc[validCross]\n",
    "    #extract diagnosis for training and validation data\n",
    "    Y_train=pd.get_dummies(training.DX,drop_first=True).to_numpy().squeeze()\n",
    "    Y_train=Y_train.tolist()\n",
    "    Y_valid=pd.get_dummies(valid.DX,drop_first=True).to_numpy().squeeze()\n",
    "    Y_valid=Y_valid.tolist()\n",
    "    #reformat training and validation datasets for pytorch\n",
    "    trainDSNew = [{\"img\": img, \"label\": label} for img, label in zip(training.filename, Y_train)]\n",
    "    validDSNew = [{\"img\": img, \"label\": label} for img, label in zip(valid.filename, Y_valid)]\n",
    "    train_ds = monai.data.Dataset(data=trainDSNew, transform=train_transforms)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=8, pin_memory=torch.cuda.is_available())\n",
    "    valid_ds = monai.data.Dataset(data=validDSNew, transform=valid_transforms)\n",
    "    valid_loader = DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=8, pin_memory=torch.cuda.is_available())\n",
    "    #set seed for reproducibility\n",
    "    set_seed(123)\n",
    "    #disable mixed precision as there are some problems with monai models\n",
    "    use_amp = False\n",
    "    #define gradient scaler\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "    #define batchsize factor used for batch accumulation --> the virtual batch size is thus 64\n",
    "    batchsize_factor=64 // BATCH_SIZE\n",
    "    #define interval in which the validation should be performed\n",
    "    val_interval = 1\n",
    "    #define list to store epoch loss values and accuracy scores per epoch\n",
    "    epoch_loss_values = []\n",
    "    #set maximal number of epochs\n",
    "    max_epochs = 100\n",
    "    set_seed(123)\n",
    "    #choose cuda as the device if it is available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    #load DL model using monai\n",
    "    model = monai.networks.nets.SEResNet152(num_classes=2,spatial_dims=3, in_channels=1)\n",
    "    #define cross entropy as loss function\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    #select optimizer\n",
    "    if opt ==\"adam\":\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    elif opt==\"sgd\":\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = torch.optim.RMSprop(model.parameters(), lr=lr)\n",
    "    #select learning rate scheduler\n",
    "    if strategy==\"step\":\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10)\n",
    "    elif strategy==\"exp\":\n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9) \n",
    "    model=model.to(device)\n",
    "    set_seed(123)\n",
    "    #iterate over epochs\n",
    "    for epoch in range(max_epochs):\n",
    "        #store starting time of epoch\n",
    "        start = time.time()\n",
    "        #start with model in train mode\n",
    "        model.train()\n",
    "        logs = {}\n",
    "        epoch_loss = 0\n",
    "        epoch_loss_val = 0\n",
    "        step = 0\n",
    "        #initialize lists to store predictions and labels of training and validation data\n",
    "        preds_epoch=[]\n",
    "        label_epoch=[]\n",
    "        preds_epoch_val=[]\n",
    "        label_epoch_val=[]\n",
    "        #iterate over training batches\n",
    "        for batch_data in train_loader:\n",
    "            step += 1\n",
    "            #load images and labels for batch\n",
    "            inputs=batch_data[\"img\"].cuda()\n",
    "            labels=batch_data[\"label\"].cuda()\n",
    "            #get model output\n",
    "            outputs = model(inputs)\n",
    "            #calculate loss\n",
    "            loss = loss_function(outputs, labels)/batchsize_factor\n",
    "            scaler.scale(loss).backward()\n",
    "            #batch accumulation\n",
    "            if (step+1) % batchsize_factor==0:\n",
    "                #update scaler and optimizer\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "            #calculate loss over epoch\n",
    "            epoch_loss += (loss.item()*batchsize_factor)\n",
    "            #extract prediction of model\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            #store prediction of model and label of subjects in the batch\n",
    "            preds_epoch.append(preds.cpu().detach().numpy())\n",
    "            label_epoch.append(labels.data.cpu().detach().numpy())\n",
    "        #increase step for scheduler\n",
    "        if strategy==\"step\":\n",
    "            scheduler.step()\n",
    "        elif strategy==\"exp\":\n",
    "            scheduler.step()\n",
    "        #store epoch training loss\n",
    "        logs['log loss'] = epoch_loss\n",
    "        #calculate and store training accuracy \n",
    "        preds_epoch = [item for sublist in preds_epoch for item in sublist]\n",
    "        label_epoch = [item for sublist in label_epoch for item in sublist]\n",
    "        logs['accuracy']=accuracy_score(label_epoch,preds_epoch)*100\n",
    "        epoch_loss_values.append(epoch_loss)\n",
    "        #model validation\n",
    "        if (epoch + 1) % val_interval == 0:\n",
    "            #change model to evaluation model\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                num_correct = 0.0\n",
    "                metric_count = 0\n",
    "                #iterate over validation data\n",
    "                for val_data in valid_loader:\n",
    "                    #load images and labels of validation batch\n",
    "                    inputs=val_data[\"img\"].cuda()\n",
    "                    labels=val_data[\"label\"].cuda()\n",
    "                    #calculate outputs of model\n",
    "                    outputs = model(inputs)\n",
    "                    #calculate loss\n",
    "                    loss=loss_function(outputs, labels)\n",
    "                    #calculate and store prediction and label for batch\n",
    "                    _, preds= torch.max(outputs, 1)\n",
    "                    preds_epoch_val.append(preds.cpu().detach().numpy())\n",
    "                    label_epoch_val.append(labels.data.cpu().detach().numpy())\n",
    "                    epoch_loss_val += loss.item()\n",
    "                #calculate and store epoch loss during validation and accuracy during validation\n",
    "                logs['val_log loss'] = epoch_loss_val\n",
    "                preds_epoch_val = [item for sublist in preds_epoch_val for item in sublist]\n",
    "                label_epoch_val = [item for sublist in label_epoch_val for item in sublist]\n",
    "                logs['val_accuracy']=accuracy_score(label_epoch_val,preds_epoch_val)*100\n",
    "        #show training and validation loss and accuracy in liveloss plot\n",
    "        liveloss.update(logs)\n",
    "        liveloss.send()\n",
    "        #save model parameters \n",
    "        torch.save(model.state_dict(),MODEL_DIR+\"model_\"+str(opt)+\"_\"+str(lr)+\"_\"+str(strategy)+\"_\"+str(cvIt)+\"_\"+str(epoch)+\".pth\")\n",
    "        #save hyperparameters and results of model\n",
    "        d = {'optimizer': [opt], 'LR': [lr],'strategy':[strategy],'CV':[cvIt], 'Epoch':[epoch], \"Epoch-Accuracy\":[accuracy_score(label_epoch_val,preds_epoch_val)*100],\"Epoch-Loss\":[epoch_loss_val]}\n",
    "        df = pd.DataFrame(data=d)\n",
    "        if os.path.isfile(filenameCSV):\n",
    "            df.to_csv(filenameCSV, mode='a', header=False)\n",
    "        else:\n",
    "            df.to_csv(filenameCSV, mode='w', header=True)\n",
    "        end = time.time()\n",
    "        #calculate time used to train one epoch\n",
    "        print(format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01a049e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculated mean and sd accuracies as well as losses for all hyperparameter combinations\n",
    "df_mean=pd.DataFrame(columns = ['optimizer', 'lr',\"strategy\", 'epoch',\"Mean ACC\",\"Mean Loss\"])\n",
    "\n",
    "for optimizer in df_filtered.optimizer.unique():\n",
    "    for lr in df_filtered.LR.unique():\n",
    "        for strategy in df_filtered.strategy.unique():\n",
    "            for epoch in df_filtered.Epoch.unique():\n",
    "                mean_value=df_filtered[((df_filtered.optimizer==optimizer)&(df_filtered.strategy==strategy)&(df_filtered.LR==lr)&(df_filtered.Epoch==epoch))][\"Epoch-Accuracy\"].mean()\n",
    "                sd_value=df_filtered[((df_filtered.optimizer==optimizer)&(df_filtered.strategy==strategy)&(df_filtered.LR==lr)&(df_filtered.Epoch==epoch))][\"Epoch-Accuracy\"].std()\n",
    "                mean_loss=df_filtered[((df_filtered.optimizer==optimizer)&(df_filtered.strategy==strategy)&(df_filtered.LR==lr)&(df_filtered.Epoch==epoch))][\"Epoch-Loss\"].mean()\n",
    "                sd_loss=df_filtered[((df_filtered.optimizer==optimizer)&(df_filtered.strategy==strategy)&(df_filtered.LR==lr)&(df_filtered.Epoch==epoch))][\"Epoch-Loss\"].std()\n",
    "                df_new=pd.DataFrame([{'optimizer' : optimizer, 'lr' : lr,'strategy': strategy, 'epoch' : epoch,\"Mean ACC\":mean_value,\"Mean Loss\":mean_loss,\"sd ACC\":sd_value,\"sd loss\":sd_loss}])\n",
    "                df_mean = pd.concat([df_mean,df_new])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb463e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#identify hyperparameter combination that achieved the best mean CV accuraacy\n",
    "df_mean=df_mean.reset_index()\n",
    "max_idx=df_mean[\"Mean ACC\"].astype(float).idxmax()\n",
    "max_obj=df_mean.iloc[max_idx]\n",
    "optimizer=max_obj[\"optimizer\"]\n",
    "lr=max_obj[\"lr\"]\n",
    "strategy=max_obj[\"strategy\"]\n",
    "epoch=max_obj[\"epoch\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a2f8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#increase number of epochs by 10 %\n",
    "epoch=int(round(epoch+1+epoch/10))-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d30fe6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate polyak models by averaging the parameters of the last 5 models for each cv iteration\n",
    "model = monai.networks.nets.SEResNet152(num_classes=2,spatial_dims=3, in_channels=1)\n",
    "PATH=MODEL_DIR+\"model_\"+str(optimizer)+\"_\"+str(lr)+\"_\"+strategy+\"_1\"+\"_\"+str(epoch)+\".pth\"\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "final_model=monai.networks.nets.SEResNet152(num_classes=2,spatial_dims=3, in_channels=1)\n",
    "beta = 0.2    \n",
    "dict_params_final_model = final_model.state_dict()\n",
    "for b in range(1,6):\n",
    "    for a in range(0,5):\n",
    "        model = monai.networks.nets.SEResNet152(num_classes=2,spatial_dims=3, in_channels=1)\n",
    "        model.load_state_dict(torch.load(MODEL_DIR+\"model_\"+str(optimizer)+\"_\"+str(lr)+\"_\"+str(strategy)+\"_\"+str(b)+\"_\"+str(epoch-a)+\".pth\"))\n",
    "        params = model.state_dict()\n",
    "        if a==0:\n",
    "            for name1 in dict_params_final_model:\n",
    "                if name1 in params:\n",
    "                    dict_params_final_model[name1]=(beta*params[name1])\n",
    "\n",
    "        else:\n",
    "            for name1 in dict_params_final_model:\n",
    "                if name1 in params:\n",
    "                    dict_params_final_model[name1]=(dict_params_final_model[name1]+beta*params[name1])\n",
    "    final_model.load_state_dict(dict_params_final_model)\n",
    "    #save model\n",
    "    torch.save(final_model.state_dict(),MODEL_DIR+\"model_\"+str(strategy)+\"_\"+str(lr)+\"_\"+str(optimizer)+\"_\"+str(b)+\"_\"+str(epoch)+\"_polyak_averaged.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3904065d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
